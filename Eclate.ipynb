{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMw7wx0td4Dh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLOgn3sKe4Hi",
        "outputId": "768d422b-d6e4-44a5-d54f-bcfa863eb484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"vstepanenko/disaster-tweets\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scy4MszKfHtL",
        "outputId": "7719fce0-aaea-4db4-bed1-04732961b03c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/vstepanenko/disaster-tweets?dataset_version_number=3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 656k/656k [00:00<00:00, 79.5MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/vstepanenko/disaster-tweets/versions/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/root/.cache/kagglehub/datasets/vstepanenko/disaster-tweets/versions/3/tweets.csv')"
      ],
      "metadata": {
        "id": "Fu1DaEBae_GI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokens(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text) # Remove URLs\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)          # Remove punctuation/numbers\n",
        "    tokens = [w for w in text.split() if w not in stop_words and len(w) > 3]\n",
        "    return list(set(tokens))"
      ],
      "metadata": {
        "id": "Q_yYA3qEfdN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transactions = df['text'].head(2000).apply(get_tokens).tolist()"
      ],
      "metadata": {
        "id": "TQ8LfUNBfhdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eclat(data, min_support, max_length):\n",
        "    # Step A: Convert to Vertical Format (Word -> Set of Transaction IDs)\n",
        "    vertical_data = {}\n",
        "    for tid, transaction in enumerate(data):\n",
        "        for item in transaction:\n",
        "            if item not in vertical_data:\n",
        "                vertical_data[item] = set()\n",
        "            vertical_data[item].add(tid)\n",
        "\n",
        "    # Filter items that meet minimum support\n",
        "    limit = len(data) * min_support\n",
        "    frequent_itemsets = {tuple([item]): tids for item, tids in vertical_data.items() if len(tids) >= limit}"
      ],
      "metadata": {
        "id": "spTTFEaEfz3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eclat(data, min_support, max_length):\n",
        "    # Step A: Convert to Vertical Format (Word -> Set of Transaction IDs)\n",
        "    vertical_data = {}\n",
        "    for tid, transaction in enumerate(data):\n",
        "        for item in transaction:\n",
        "            if item not in vertical_data:\n",
        "                vertical_data[item] = set()\n",
        "            vertical_data[item].add(tid)\n",
        "\n",
        "    # Filter items that meet minimum support\n",
        "    limit = len(data) * min_support\n",
        "    frequent_itemsets = {tuple([item]): tids for item, tids in vertical_data.items() if len(tids) >= limit}\n",
        "\n",
        "    # Step B: Recursive Intersections\n",
        "    items = sorted(list(frequent_itemsets.keys()))\n",
        "\n",
        "    def recurse(current_itemsets, level):\n",
        "        if level > max_length:\n",
        "            return\n",
        "\n",
        "        new_itemsets = {}\n",
        "        items_list = list(current_itemsets.keys())\n",
        "\n",
        "        for i in range(len(items_list)):\n",
        "            for j in range(i + 1, len(items_list)):\n",
        "                # Join itemsets if they share the same prefix\n",
        "                it1, it2 = items_list[i], items_list[j]\n",
        "                if it1[:-1] == it2[:-1]:\n",
        "                    combined_itemset = it1 + (it2[-1],)\n",
        "                    # Intersect TID sets\n",
        "                    intersection_tids = current_itemsets[it1].intersection(current_itemsets[it2])\n",
        "\n",
        "                    if len(intersection_tids) >= limit:\n",
        "                        new_itemsets[combined_itemset] = intersection_tids\n",
        "                        frequent_itemsets[combined_itemset] = intersection_tids\n",
        "\n",
        "        if new_itemsets:\n",
        "            recurse(new_itemsets, level + 1)\n",
        "\n",
        "    recurse(frequent_itemsets, 2)\n",
        "    return frequent_itemsets"
      ],
      "metadata": {
        "id": "rHYDRfeKf38e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_sup = 0.01\n",
        "results = eclat(transactions, min_support=min_sup, max_length=2)"
      ],
      "metadata": {
        "id": "FqXaN_6kgCiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"--- Frequent Word Associations (Support > {min_sup*100}%) ---\")\n",
        "# Filter for pairs (length 2) to see associations\n",
        "associations = {k: len(v) for k, v in results.items() if len(k) > 1}\n",
        "# Sort by frequency\n",
        "sorted_associations = sorted(associations.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "for words, count in sorted_associations[:15]:\n",
        "    print(f\"Words: {words} | Frequency: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YESSAz0Agkvg",
        "outputId": "77a15a86-78ec-48c6-881c-0fe570b761d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Frequent Word Associations (Support > 1.0%) ---\n",
            "Words: ('fires', 'bush') | Frequency: 45\n",
            "Words: ('body', 'bags') | Frequency: 42\n",
            "Words: ('burning', 'buildings') | Frequency: 34\n",
            "Words: ('accident', 'airplane') | Frequency: 32\n",
            "Words: ('fire', 'buildings') | Frequency: 29\n",
            "Words: ('need', 'battle') | Frequency: 24\n",
            "Words: ('body', 'bagging') | Frequency: 24\n",
            "Words: ('need', 'backup') | Frequency: 23\n",
            "Words: ('battle', 'backup') | Frequency: 23\n",
            "Words: ('bridge', 'collapse') | Frequency: 23\n",
            "Words: ('body', 'parts') | Frequency: 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Associations with the word 'fire' ---\")\n",
        "for words, count in associations.items():\n",
        "    if 'body' in words:\n",
        "        print(f\"{words}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVb2BmE-gngh",
        "outputId": "528d9074-edce-4a9d-f804-e0fc4b0ef272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Associations with the word 'fire' ---\n",
            "('body', 'parts'): 21\n",
            "('body', 'bags'): 42\n",
            "('body', 'bagging'): 24\n"
          ]
        }
      ]
    }
  ]
}